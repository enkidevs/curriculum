---
author: nemanjaenki
type: normal
category: must-know
practiceQuestion:
  formats:
    - fill-in-the-gap
  context: standalone
revisionQuestion:
  formats:
    - fill-in-the-gap
  context: standalone
---

# ðŸ‘€ Attention Heads

---

## Content

Each attention layer has multiple *attention heads*, each focusing on different tasks.

For example:
- One head links pronouns to nouns.
- Another resolves homonyms like "bat" (animal vs. sports object).
- A third connects phrases like "New York" to the city vs the state.

The results of one head often feed into the next, building deeper understanding.

---

## Practice

The name for contextual trackers that help LLMs understand the text is ???.

- attention heads
- attention layers
- attention vectors
- attention keys

---

## Revision

Each attention layer has multiple attention ???, each focusing on different tasks, trying to contextualize words with relationships to other words.

- heads
- skulls
- vectors
- keys
