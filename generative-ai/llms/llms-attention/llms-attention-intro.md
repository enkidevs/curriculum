---
author: nemanjaenki
type: normal
category: must-know
---

# ðŸ‘€ Attention

---

## Content

In the attention step, each word "looks around" for other words in the text to find relevant context and exchange information.

For example, in `"Mary saw her friend at the park,"` the attention step links `"her"` to `"Mary"` by matching their characteristics.

Since each word can test out other words independently, this approach enables LLMs to handle massive texts efficiently because all words can be analyzed in parallel.

> ðŸš€ This is why GPUs are used for LLMs - they can perform massive amounts of analytical operations in parallel.