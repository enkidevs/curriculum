---
author: nemanjaenki
type: normal
category: must-know
---

# Attention and Feed-Forward

---

## Content

Think of attention heads as the model's short-term memory - they look back at words you just typed. 

Feed-forward layers are like the model's long-term memory - they store everything the model learned during training.

The feed-forward layers are like a giant library of knowledge.

The first few shelves have simple facts, like `"peanut butter goes well with jelly"` or `"cats say meow"`. 

As you go deeper into the library, you find more complex knowledge, like `"if you add wings to a horse, you get a pegasus!"`.

