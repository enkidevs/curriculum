---
author: nemanjaenki
type: normal
category: must-know
---

# ğŸ“ˆ Bigger Models, Better Reasoning

---

## Content

Scaling the LLM's network size improves both the ability to memorize facts and perform abstract reasoning.


```
GPT-1: 117 million parameters.
GPT-2: 1.5 billion parameters.
GPT-3: 175 billion parameters.
GPT-4: 1700 billion parameters.
```

Larger models are better at tasks like analogy and inference.

> ğŸ’¡ bigger LLM + more data = better performance.
