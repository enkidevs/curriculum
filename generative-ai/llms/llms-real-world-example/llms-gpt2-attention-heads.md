---
author: nemanjaenki
type: normal
category: must-know
---

# ðŸ§© GPT-2 Attention Heads

GPT-2's prediction process involved layers of reasoning:

- Duplicate Token Heads marked the second `"John"` as a repeat.
- Subject Inhibition Heads used this to block copying `"John".`
- Name Mover Heads copied `"Mary"` instead.

This chain of steps allowed the model to infer the next logical name.

> ðŸ’« GPT-2's prediction process involved attention heads that marked repeats,
> blocked duplicates, and copied names.
