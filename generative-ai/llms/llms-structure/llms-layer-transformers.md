---
author: nemanjaenki
type: normal
category: must-know
practiceQuestion:
  formats:
    - fill-in-the-gap
  context: standalone
revisionQuestion:
  formats:
    - fill-in-the-gap
  context: standalone
links:
  - >-
    [Attention is All You Need](https://arxiv.org/abs/1706.03762)
---

# ðŸ§  Layer Transformers

---

## Content

Each layer of an LLM is actually a **transformer**.

![llm-layer-transformers](https://img.enkipro.com/af1e642f858c2d83d8c4c210d2fd93a0.png)

A transformer is a neural network architecture that was first introduced by Google in a landmark 2017 paper called "Attention is All You Need".

Let's dive more into this idea of "attention" in the next workout.

---

## Practice

The architecture of an LLM layer is a ???

- transformer
- transponder
- tricoder
- informer

---

## Revision

Each layer of an LLM is a ???

- transformer
- transponder
- tricoder
- informer
