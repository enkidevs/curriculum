---
author: nemanjaenki
type: normal
category: must-know
practiceQuestion:
  formats:
    - fill-in-the-gap
  context: standalone
revisionQuestion:
  formats:
    - fill-in-the-gap
  context: standalone
---

# ðŸ§  Hidden Layers

---

## Content

There can be many layers in an LLM between the input text and the output prediction.

These internal layers are called **hidden layers**.

As more layers process the text, the model builds a detailed mental "note" of the context for each word.

Early layers focus on syntax and resolving ambiguities, while later layers interpret broader context.

---

## Practice

With each layer, the model's understanding of the sentence doesn't change.

???

- false
- true

---

## Revision

LLMs are organized into layers because ???

- each layer refines the meaning of words in context
- layers hide the internal workings of the model
- layers process the input word by word
- each layer is easier to understand
